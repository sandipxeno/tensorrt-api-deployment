{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime-gpu\n",
        "!pip install psutil"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ub8o5Vf6p1z",
        "outputId": "a9ca3ab1-3a6b-4de8-e18f-dbd757338cbc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnxruntime-gpu\n",
            "  Downloading onnxruntime_gpu-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Collecting coloredlogs (from onnxruntime-gpu)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (25.2.10)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu) (1.13.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime-gpu)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime-gpu) (1.3.0)\n",
            "Downloading onnxruntime_gpu-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (280.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.8/280.8 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime-gpu\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-gpu-1.21.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (5.9.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import psutil\n",
        "import numpy as np\n",
        "import onnxruntime as ort\n",
        "from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo\n",
        "\n",
        "# Initialize NVML for GPU memory tracking\n",
        "nvmlInit()\n",
        "device_handle = nvmlDeviceGetHandleByIndex(0)  # Assuming single GPU\n",
        "\n",
        "# Load ONNX model with GPU support\n",
        "onnx_session = ort.InferenceSession(\"/content/resnet50_dog_cat.onnx\", providers=[\"CUDAExecutionProvider\"])\n",
        "\n",
        "# Define input shape\n",
        "IMG_SIZE = 224  # Modify this if needed\n",
        "\n",
        "def preprocess_dummy_image():\n",
        "    \"\"\"Create a dummy image and preprocess it to match model input.\"\"\"\n",
        "    image = np.random.rand(IMG_SIZE, IMG_SIZE, 3).astype(np.float32)  # Random image\n",
        "    image = image / 255.0  # Normalize\n",
        "    image = np.transpose(image, (2, 0, 1))  # Convert to (C, H, W)\n",
        "    image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
        "    return image\n",
        "\n",
        "def benchmark_onnx_runtime(num_iterations=100):\n",
        "    \"\"\"Benchmark ONNX inference speed, GPU, and memory usage.\"\"\"\n",
        "    latencies = []\n",
        "    gpu_memory_usages = []\n",
        "    input_name = onnx_session.get_inputs()[0].name\n",
        "\n",
        "    print(f\"Running {num_iterations} inferences on ONNX Runtime (GPU)...\")\n",
        "\n",
        "    for _ in range(num_iterations):\n",
        "        # Preprocess input\n",
        "        image = preprocess_dummy_image()\n",
        "\n",
        "        # Measure GPU memory before inference\n",
        "        gpu_memory_before = nvmlDeviceGetMemoryInfo(device_handle).used / (1024 * 1024)  # Convert bytes to MB\n",
        "\n",
        "        # Measure inference time\n",
        "        start_time = time.time()\n",
        "        _ = onnx_session.run(None, {input_name: image})\n",
        "        end_time = time.time()\n",
        "\n",
        "        # Measure GPU memory after inference\n",
        "        gpu_memory_after = nvmlDeviceGetMemoryInfo(device_handle).used / (1024 * 1024)\n",
        "\n",
        "        # Store metrics\n",
        "        latencies.append((end_time - start_time) * 1000)  # Convert to ms\n",
        "        gpu_memory_usages.append(gpu_memory_after - gpu_memory_before)\n",
        "\n",
        "    # Compute statistics\n",
        "    avg_latency = np.mean(latencies)\n",
        "    max_latency = np.max(latencies)\n",
        "    min_latency = np.min(latencies)\n",
        "    avg_gpu_memory_usage = np.mean(gpu_memory_usages)\n",
        "    throughput = num_iterations / (sum(latencies) / 1000)  # Inferences per second\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\n==== ONNX Runtime (GPU) Benchmark Results ====\")\n",
        "    print(f\"Average Latency: {avg_latency:.2f} ms\")\n",
        "    print(f\"Min Latency: {min_latency:.2f} ms\")\n",
        "    print(f\"Max Latency: {max_latency:.2f} ms\")\n",
        "    print(f\"Average GPU Memory Usage: {avg_gpu_memory_usage:.2f} MB\")\n",
        "    print(f\"Throughput: {throughput:.2f} inferences per second\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    benchmark_onnx_runtime(num_iterations=100)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWp0mYx_6J_h",
        "outputId": "f5f8a50c-1749-4501-c829-057818f70f67"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running 100 inferences on ONNX Runtime (GPU)...\n",
            "\n",
            "==== ONNX Runtime (GPU) Benchmark Results ====\n",
            "Average Latency: 24.12 ms\n",
            "Min Latency: 4.32 ms\n",
            "Max Latency: 1659.68 ms\n",
            "Average GPU Memory Usage: 1.38 MB\n",
            "Throughput: 41.46 inferences per second\n"
          ]
        }
      ]
    }
  ]
}