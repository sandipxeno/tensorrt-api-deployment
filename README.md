## tensorrt-api-deployment ##
This project focuses on creating an optimized inference pipeline that could run seamlessly on both CPUs and GPUs while maintaining high performance.This file includes:
Selecting an appropriate model format - ONNX for its cross-platform compatibility. 
Exploring multiple deployment strategies.
Testing inference performance on CPU using ONNX Runtime and OpenVINO.
Testing inference performance on GPU using CUDA and TensorRT. 
Benchmarking their performance.